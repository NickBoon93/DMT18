{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParams\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete example using RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/benhamner/ExpediaPersonalizedSortCompetition/blob/master/PythonBenchmark/train.py\n",
    "df = pd.read_csv('../../training_set_VU_DM_2014.csv').fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove features not available in test set\n",
    "feature_names = list(df.columns)\n",
    "feature_names.remove(\"click_bool\")\n",
    "feature_names.remove(\"booking_bool\")\n",
    "feature_names.remove(\"gross_bookings_usd\")\n",
    "feature_names.remove(\"date_time\")\n",
    "feature_names.remove(\"position\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into features and target\n",
    "features = df[feature_names].values\n",
    "target = df[\"booking_bool\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Classifier\n",
      "building tree 1 of 50\n",
      "building tree 2 of 50building tree 3 of 50building tree 4 of 50\n",
      "\n",
      "\n",
      "building tree 5 of 50\n",
      "building tree 6 of 50\n",
      "building tree 7 of 50\n",
      "building tree 8 of 50\n",
      "building tree 9 of 50\n",
      "building tree 10 of 50\n",
      "building tree 11 of 50\n",
      "building tree 12 of 50\n",
      "building tree 13 of 50\n",
      "building tree 14 of 50\n",
      "building tree 15 of 50\n",
      "building tree 16 of 50\n",
      "building tree 17 of 50\n",
      "building tree 18 of 50\n",
      "building tree 19 of 50\n",
      "building tree 20 of 50\n",
      "building tree 21 of 50\n",
      "building tree 22 of 50\n",
      "building tree 23 of 50\n",
      "building tree 24 of 50\n",
      "building tree 25 of 50\n",
      "building tree 26 of 50\n",
      "building tree 27 of 50\n",
      "building tree 28 of 50\n",
      "building tree 29 of 50\n",
      "building tree 30 of 50\n",
      "building tree 31 of 50\n",
      "building tree 32 of 50\n",
      "building tree 33 of 50\n",
      "building tree 34 of 50\n",
      "building tree 35 of 50\n",
      "building tree 36 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:  4.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 37 of 50\n",
      "building tree 38 of 50\n",
      "building tree 39 of 50\n",
      "building tree 40 of 50\n",
      "building tree 41 of 50\n",
      "building tree 42 of 50\n",
      "building tree 43 of 50\n",
      "building tree 44 of 50\n",
      "building tree 45 of 50\n",
      "building tree 46 of 50\n",
      "building tree 47 of 50\n",
      "building tree 48 of 50\n",
      "building tree 49 of 50\n",
      "building tree 50 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:  6.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=10,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=4,\n",
       "            oob_score=False, random_state=1, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "print(\"Training the Classifier\")\n",
    "classifier = RandomForestClassifier(n_estimators=50, \n",
    "                                        verbose=2,\n",
    "                                        n_jobs=4,\n",
    "                                        min_samples_split=10,\n",
    "                                        random_state=1)\n",
    "classifier.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save model\n",
    "# filename = 'model.sav'\n",
    "# pickle.dump(classifier, open(filename, 'wb'))\n",
    "\n",
    "# # load model\n",
    "# loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set\n",
    "test = pd.read_csv('../../test_set_VU_DM_2014.csv').fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:   26.2s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:   35.3s finished\n"
     ]
    }
   ],
   "source": [
    "# obtain feature names and remove useless column date_time\n",
    "feature_names = list(test.columns)\n",
    "feature_names.remove(\"date_time\")\n",
    "\n",
    "# obtain feature values\n",
    "features = test[feature_names].values\n",
    "\n",
    "# predict using trained model\n",
    "print(\"Making predictions\")\n",
    "predictions = classifier.predict_proba(features)[:,1]\n",
    "predictions = list(-1.0*predictions)\n",
    "recommendations = zip(test[\"srch_id\"], test[\"prop_id\"], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort based on recommendation value and write sorted prediction to file\n",
    "\n",
    "from operator import itemgetter\n",
    "import csv\n",
    "\n",
    "rows = [(srch_id, prop_id)\n",
    "        for srch_id, prop_id, rank_float\n",
    "        in sorted(recommendations, key=itemgetter(0,2))]\n",
    "writer = csv.writer(open(\"predict.csv\", \"w\"), lineterminator=\"\\n\")\n",
    "writer.writerow((\"SearchId\", \"PropertyId\"))\n",
    "writer.writerows(rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
