{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParams\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import datetime\n",
    "from operator import itemgetter\n",
    "import csv\n",
    "\n",
    "pd.set_option('display.max_columns',75)\n",
    "pd.set_option('display.max_rows',75)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sites to look at\n",
    "\n",
    "https://www.dataquest.io/blog/kaggle-tutorial/\n",
    "\n",
    "Need one-hot encoding for our categorical data if using scikit randomforest\n",
    "\n",
    "https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../training_set_VU_DM_2014.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert time-fields to usable features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date_time(df):\n",
    "    #convert date_time to datetime\n",
    "    df[\"date_time\"] = pd.to_datetime(df[\"date_time\"])\n",
    "\n",
    "    #add column for the starting date of the booking\n",
    "    df[\"book_start\"] = df[\"date_time\"] + pd.to_timedelta(df['srch_booking_window'], unit='D')\n",
    "    df[\"book_end\"] = df[\"book_start\"] + pd.to_timedelta(df['srch_length_of_stay'], unit='D')\n",
    "\n",
    "    #extract usable features\n",
    "    df[\"srch_weekday\"] = df[\"date_time\"].dt.weekday\n",
    "    df[\"srch_month\"] = df[\"date_time\"].dt.month\n",
    "    df[\"srch_quarter\"] = df[\"date_time\"].dt.quarter\n",
    "    df[\"srch_year\"] = df[\"date_time\"].dt.year\n",
    "    df[\"book_start_weekday\"] = df[\"book_start\"].dt.weekday\n",
    "    df[\"book_start_month\"] = df[\"book_start\"].dt.month\n",
    "    df[\"book_start_quarter\"] = df[\"book_start\"].dt.quarter\n",
    "    df[\"book_start_year\"] = df[\"book_start\"].dt.year\n",
    "    df[\"book_end_weekday\"] = df[\"book_end\"].dt.weekday\n",
    "    df[\"book_end_month\"] = df[\"book_end\"].dt.month\n",
    "    df[\"book_end_quarter\"] = df[\"book_end\"].dt.quarter\n",
    "    df[\"book_end_year\"] = df[\"book_end\"].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_date_time(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some feature engineering with competitor columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_comp_columns(df):\n",
    "    #Take minimum of comp#_rate columns. If Expedia is cheaper than all competitors (all are 1), this will be 1.\n",
    "    #If one competitor is cheaper (-1), this column will equal -1 (and have less chance of being booked at Expedia!)\n",
    "    df[\"comp\"] = df[[\"comp%d_rate\"%i for i in range(1,9)]].min(axis=1)\n",
    "    \n",
    "    #multiply comp_rate and comp_rate_percent_diff and drop the old columns\n",
    "    for i in range(1,9):\n",
    "        df[\"comp%d\"%i] = df[\"comp%d_rate\"%i] * df[\"comp%d_rate_percent_diff\"%i]\n",
    "        df.drop([\"comp%d_rate\"%i, \"comp%d_rate_percent_diff\"%i],axis=1,inplace=True)\n",
    "\n",
    "def add_averages_and_diffs(df):\n",
    "    #https://stackoverflow.com/a/30949063\n",
    "    df['avg_prop_starrating'] = df.groupby('srch_id')['prop_starrating'].transform('mean')\n",
    "    df['avg_prop_location_score1'] = df.groupby('srch_id')['prop_location_score1'].transform('mean')\n",
    "    df['avg_prop_location_score2'] = df.groupby('srch_id')['prop_location_score2'].transform('mean')\n",
    "    df['avg_price_usd'] = df.groupby('srch_id')['price_usd'].transform('mean')\n",
    "\n",
    "    #create difference columns, comparing the average with the value of each row\n",
    "    df['avg_prop_starrating_diff'] = df['prop_starrating'] - df['avg_prop_starrating']\n",
    "    df['avg_prop_location_score1_diff'] = df['prop_location_score1'] - df['avg_prop_location_score1']\n",
    "    df['avg_prop_location_score2_diff'] = df['prop_location_score2'] - df['avg_prop_location_score2']\n",
    "    df['avg_price_usd_diff'] = df['avg_price_usd'] - df['price_usd'] #cheaper is better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_comp_columns(df)\n",
    "add_averages_and_diffs(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-FOLD CROSS-VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split training set into training and test set for evaluating performance\n",
    "\n",
    "We don't have booking_bool in the test set provided, so we can do this to estimate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TOTAL SEARCHES IN DATASET: %d\"%len(df[\"srch_id\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#perform 10-fold cross-validation over a subset of the data\n",
    "splits = 10\n",
    "\n",
    "#get list of unique ids\n",
    "ids = df[\"srch_id\"].unique()\n",
    "\n",
    "#shuffle the ids randomly\n",
    "np.random.shuffle(ids)\n",
    "\n",
    "#and obtain a list of test_ids per split\n",
    "ids = ids[0:int(len(ids)/splits)*splits] #drop some srch_ids to keep test set sizes equal\n",
    "split_ids = np.split(ids,splits)\n",
    "\n",
    "#init list to store results per split\n",
    "result = []\n",
    "\n",
    "for n,i in enumerate(split_ids):\n",
    "    \n",
    "    #obtain test_set from ids, and training_set from the other ids\n",
    "    test_set = df.loc[df[\"srch_id\"].isin(i)]\n",
    "    training_set = df.loc[~(df[\"srch_id\"].isin(i)) & (df[\"srch_id\"].isin(ids))]\n",
    "\n",
    "    #fill NaNs in both sets with zero\n",
    "    test_set.fillna(0,inplace=True)\n",
    "    training_set.fillna(0,inplace=True)\n",
    "    \n",
    "    #obtain list of feature names to train model on\n",
    "    feature_names = list(training_set.columns)\n",
    "    feature_names.remove(\"date_time\")\n",
    "    feature_names.remove(\"book_start\")\n",
    "    feature_names.remove(\"book_end\")\n",
    "    feature_names.remove(\"position\")\n",
    "    feature_names.remove(\"click_bool\")\n",
    "    feature_names.remove(\"gross_bookings_usd\")\n",
    "    feature_names.remove(\"random_bool\")\n",
    "    feature_names.remove(\"booking_bool\")\n",
    "    \n",
    "    #obtain feature values from training set\n",
    "    features = training_set[feature_names].values\n",
    "    \n",
    "    #obtain target values from training set\n",
    "    target = training_set[\"booking_bool\"].values\n",
    "    \n",
    "    #initialize model\n",
    "    classifier = RandomForestClassifier(n_estimators=50, \n",
    "                                        verbose=1,\n",
    "                                        n_jobs=4,\n",
    "                                        min_samples_split=10,\n",
    "                                        random_state=1)\n",
    "    #train model\n",
    "    classifier.fit(features, target)\n",
    "    \n",
    "    #obtain list of feature names for the test set\n",
    "    feature_names = list(test_set.columns)\n",
    "    feature_names.remove(\"date_time\")\n",
    "    feature_names.remove(\"book_start\")\n",
    "    feature_names.remove(\"book_end\")\n",
    "    feature_names.remove(\"position\")\n",
    "    feature_names.remove(\"click_bool\")\n",
    "    feature_names.remove(\"gross_bookings_usd\")\n",
    "    feature_names.remove(\"random_bool\")\n",
    "    feature_names.remove(\"booking_bool\")\n",
    "    \n",
    "    #get feature values from the test set\n",
    "    features = test_set[feature_names].values\n",
    "    \n",
    "    #make predictions\n",
    "    predictions = classifier.predict_proba(features)[:,1]\n",
    "    predictions = list(-1.0*predictions)\n",
    "    recommendations = zip(test_set[\"srch_id\"], test_set[\"prop_id\"],\\\n",
    "                          test_set[\"booking_bool\"], test_set[\"click_bool\"],\\\n",
    "                          predictions)\n",
    "    \n",
    "    #generate the sorted rows\n",
    "    rows = [(srch_id, prop_id,booking_bool,click_bool)\n",
    "        for srch_id, prop_id,booking_bool,click_bool, rank_float\n",
    "        in sorted(recommendations, key=itemgetter(0,4))]\n",
    "    \n",
    "    #write sorted rows to file\n",
    "    with open(\"predict%d.csv\"%n, \"w\") as outfile:\n",
    "        writer = csv.writer(outfile, lineterminator=\"\\n\")\n",
    "        writer.writerow((\"SearchId\", \"PropertyId\", \"BookingBool\",\"ClickBool\"))\n",
    "        writer.writerows(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance(row):\n",
    "    \"\"\"\n",
    "    Returns the relevance score per row\n",
    "    \"\"\"\n",
    "    if row[\"BookingBool\"]:\n",
    "        return 5\n",
    "    elif row[\"ClickBool\"]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def dcg_at_k(r, k):\n",
    "    \"\"\"\n",
    "    Calculates the discounted cumulative gain as per Kaggle's definition\n",
    "    \n",
    "    Returns the DCG@k\n",
    "    \n",
    "    Code adjusted from https://www.kaggle.com/wendykan/ndcg-example\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        return np.sum((2 ** r -1)/ np.log2(np.arange(2, r.size + 2)))\n",
    "    return 0.\n",
    "\n",
    "\n",
    "def ndcg_at_k(r, k):\n",
    "    \"\"\"\n",
    "    Calculates the normalized discounted cumulative gain as per\n",
    "    Kaggle's definition\n",
    "    \n",
    "    Returns the NDCG@k\n",
    "    \n",
    "    Code adjusted from https://www.kaggle.com/wendykan/ndcg-example\n",
    "    \"\"\"\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k) / dcg_max\n",
    "\n",
    "#initialize list to store score per split\n",
    "score = []\n",
    "\n",
    "for n in range(splits):\n",
    "    \n",
    "    #read predict file for this split\n",
    "    result = pd.read_csv('predict%d.csv'%n)\n",
    "    \n",
    "    #add relevance score\n",
    "    result['rel'] = result.apply(relevance,axis=1)\n",
    "    \n",
    "    #initialize score at zero\n",
    "    score_n = 0\n",
    "    \n",
    "    #loop over all srch_id\n",
    "    for srch_id in result[\"SearchId\"].unique():\n",
    "        #use NDCG@38 as per Kaggle site\n",
    "        score_n += ndcg_at_k(result.loc[result[\"SearchId\"]==srch_id,\"rel\"].values,38)\n",
    "    \n",
    "    #append averaged score to final list\n",
    "    score.append(score_n / len(result[\"SearchId\"].unique()))\n",
    "print(score)\n",
    "print(\"AVERAGED SCORE: %s\"%np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretty print scores per split\n",
    "for n,i in enumerate(score):\n",
    "    print(\"SPLIT %d: %.16f\"%(n,i))\n",
    "    \n",
    "#print final score with 95% confidence intervals\n",
    "print(\"AVERAGED SCORE: %.16f +/- %.16f\"%(np.mean(score),1.96*np.std(score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATING PREDICTION FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill all NaNs with zeros\n",
    "df.fillna(0,inplace=True)\n",
    "    \n",
    "#obtain feature names\n",
    "feature_names = list(df.columns)\n",
    "#model cannot train on DateTime\n",
    "feature_names.remove(\"date_time\")\n",
    "feature_names.remove(\"book_start\")\n",
    "feature_names.remove(\"book_end\")\n",
    "#remove columns not available in test set\n",
    "feature_names.remove(\"position\")\n",
    "feature_names.remove(\"click_bool\")\n",
    "feature_names.remove(\"gross_bookings_usd\")\n",
    "feature_names.remove(\"booking_bool\")\n",
    "#remove useless variable (without position)\n",
    "feature_names.remove(\"random_bool\")\n",
    "\n",
    "#obtain feature values\n",
    "features = df[feature_names].values\n",
    "\n",
    "#obtain target values\n",
    "target = df[\"booking_bool\"].values\n",
    "\n",
    "#remove training set from memory as it is no longer needed\n",
    "del df\n",
    "\n",
    "#initialize RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=50, \n",
    "                                    verbose=2,\n",
    "                                    n_jobs=4,\n",
    "                                    min_samples_split=10,\n",
    "                                    random_state=1)\n",
    "#train model\n",
    "classifier.fit(features, target)\n",
    "\n",
    "#remove features, target from memory as it is no longer needed\n",
    "del features\n",
    "del target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load test set and prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load test set\n",
    "test = pd.read_csv('../../test_set_VU_DM_2014.csv')\n",
    "\n",
    "#add features like in training set\n",
    "convert_date_time(test)\n",
    "add_comp_columns(test)\n",
    "add_averages_and_diffs(test)\n",
    "\n",
    "#fill all NaNs with zeros\n",
    "test.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain feature names\n",
    "feature_names = list(test.columns)\n",
    "\n",
    "#remove DateTime columns\n",
    "feature_names.remove(\"date_time\")\n",
    "feature_names.remove(\"book_start\")\n",
    "feature_names.remove(\"book_end\")\n",
    "\n",
    "#remove useless variable (without position)\n",
    "feature_names.remove(\"random_bool\")\n",
    "\n",
    "# obtain feature values\n",
    "features = test[feature_names].values\n",
    "\n",
    "# predict using trained model\n",
    "predictions = classifier.predict_proba(features)[:,1]\n",
    "predictions = list(-1.0*predictions)\n",
    "recommendations = zip(test[\"srch_id\"], test[\"prop_id\"], predictions)\n",
    "\n",
    "#remove test set from memory\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort based on recommendation value\n",
    "rows = [(srch_id, prop_id)\n",
    "        for srch_id, prop_id, rank_float\n",
    "        in sorted(recommendations, key=itemgetter(0,2))]\n",
    "\n",
    "#write sorted prediction to file\n",
    "with open(\"final_predict.csv\", \"w\") as outfile:\n",
    "    writer = csv.writer(outfile, lineterminator=\"\\n\")\n",
    "    writer.writerow((\"SearchId\", \"PropertyId\"))\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### show first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### show column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### show column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### show short description per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[\"srch_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"srch_id\").count()[\"date_time\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### show correlations with the target booking_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()[\"booking_bool\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr(df,size=10):\n",
    "    '''Function plots a graphical correlation matrix for each pair of columns in the dataframe.\n",
    "\n",
    "    Input:\n",
    "        df: pandas DataFrame\n",
    "        size: vertical and horizontal size of the plot'''\n",
    "\n",
    "    corr = df.corr()\n",
    "    fig, ax = plt.subplots(figsize=(size, size))\n",
    "    cax = ax.matshow(corr)\n",
    "    fig.colorbar(cax)\n",
    "    ax.matshow(corr)\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns);\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr(df,25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### comp#_inv has some weird values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what are -1 values? Expedia has no availability, but competitor does? Why would Expedia then show the hotel?\n",
    "#(Create new feature based on availablility?)\n",
    "print(df[\"comp1_inv\"].value_counts())\n",
    "print(\"Number of NaNs: %d\"%df[\"comp1_inv\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Not all searches lead to a booking, but all do have a clicked item!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_book = set(df.loc[df[\"booking_bool\"]==1,\"srch_id\"].unique())\n",
    "ids = set(df[\"srch_id\"].unique())\n",
    "print(\"Number of unique searches: %d\"%len(ids))\n",
    "print(\"Number of unique searches resulting in booking: %d\"%len(id_book))\n",
    "print(\"Number of unique searches without booking: %d\"%len(ids-id_book))\n",
    "id_click = set(df.loc[df[\"click_bool\"]==1,\"srch_id\"].unique())\n",
    "print(\"Number of unique searches with clicks: %d\"%len(id_click))\n",
    "print(\"Number of unique searches without clicks: %d\"%len(ids-id_click))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get N largest values from column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[np.argsort(df[\"price_usd\"].values)[-1:-10:-1],[\"srch_id\",\"price_usd\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#this search seems to have incorrect price_usd values, as all values have no cents?\n",
    "df.loc[df[\"srch_id\"]==78107]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### show histogram of all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.hist(df.columns.values,figsize=(8,10*len(df.columns)),layout=(len(df.columns),1));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
