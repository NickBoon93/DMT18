{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.springboard.com/blog/data-mining-python-tutorial/\n",
    "\n",
    "http://www.developintelligence.com/blog/2017/08/data-cleaning-pandas-python/\n",
    "\n",
    "\n",
    "https://machinelearningmastery.com/quick-and-dirty-data-analysis-with-pandas/\n",
    "\n",
    "Example of simple classification problem:\n",
    "https://towardsdatascience.com/solving-a-simple-classification-problem-with-python-fruits-lovers-edition-d20ab6b071d2\n",
    "\n",
    "Cross-validation in sklearn\n",
    "http://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParams\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load csv and create a copy to store cleaned values\n",
    "df = pd.read_csv('ODI-2018.csv',skiprows=[1]) #skip first (empty) row\n",
    "clean = df.copy()\n",
    "\n",
    "#show first 10 responses\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for null values\n",
    "df.isnull().any()\n",
    "\n",
    "#Due to skipping the first empty row, and the questionnaire requiring an\n",
    "#answer for every question, no null values exist in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for value types\n",
    "df.dtypes\n",
    "\n",
    "# No validation was used in the questionnaire, so every answer is of object-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show some statistics of the dataset\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Clean first column\n",
    "#method taken from:\n",
    "#https://stackoverflow.com/questions/20250771/remap-values-in-pandas-column-with-a-dict\n",
    "data = df[df.columns[1]].str.lower()     #lowercase\n",
    "data = data.str.strip()                  #remove space left and right of string\n",
    "data = data.str.replace('&','and')       #replace & by and\n",
    "clean[df.columns[1]] = data.replace(\n",
    "    {'21-05-1995':np.NaN,\n",
    "    'a. i.':'Artificial Intelligence',\n",
    "    'ai':'Artificial Intelligence',\n",
    "    'ai (cognitive sciences)':'Artificial Intelligence',\n",
    "    'ai (vu version)':'Artificial Intelligence',\n",
    "    'ai vu':'Artificial Intelligence',\n",
    "    'ai premaster':'Artificial Intelligence',\n",
    "    'artificial intelligence (socially aware computing)':'Artificial Intelligence',\n",
    "    'artificial intelligence':'Artificial Intelligence',\n",
    "    'ba':'Business Analytics',\n",
    "    'b science, business and innovation':'Science, Business & Innovation',\n",
    "    'big data engineering':'Computer Science',\n",
    "    'bioinformatcis':'Bioinformatics & Systems Biology',\n",
    "    'bioinformatics and system biology':'Bioinformatics & Systems Biology',\n",
    "    'bioinformatics':'Bioinformatics & Systems Biology',\n",
    "    'bioinformatics and systems biology':'Bioinformatics & Systems Biology',\n",
    "    'bioinformatics and sysbio':'Bioinformatics & Systems Biology',\n",
    "    'bioinformatics master':'Bioinformatics & Systems Biology',\n",
    "    'business analytics msc':'Business Analytics',\n",
    "    'business analytics/ operations research':'Business Analytics',\n",
    "    'business analytics':'Business Analytics',\n",
    "    'cls':'Computational Science',\n",
    "    'cs':'Computer Science',\n",
    "    'csl':'Computational Science',\n",
    "    'comoputational science':'Computational Science',\n",
    "    'computational science (jd)':'Computational Science',\n",
    "    'computational science':'Computational Science',\n",
    "    'computer science':'Computer Science',\n",
    "    'computer science: big data engineering':'Computer Science',\n",
    "    'data mining techniques':np.NaN,\n",
    "    'drug discovery and safety':'Drug Discovery and Safety',\n",
    "    'duisenberg honors program quantitative risk managament':'Quantitative Risk Management',\n",
    "    'duisenberg quantitative risk management':'Quantitative Risk Management',\n",
    "    'econometrics':'Econometrics',\n",
    "    'econometrics and operations research':'Econometrics and Operations Research',\n",
    "    'economics':'Economics',\n",
    "    'eor':'Econometrics and Operations Research',\n",
    "    'exchange':'Exchange student',\n",
    "    'finance':'Finance',\n",
    "    'finance dhp qrm':'Quantitative Risk Management',\n",
    "    'm financial economtrics':'Econometrics',\n",
    "    'ma bioinformatics':'Bioinformatics & Systems Biology',\n",
    "    'master bionformatics and systems biology':'Bioinformatics & Systems Biology',\n",
    "    'master business analytics':'Business Analytics',\n",
    "    'master computer science: big data engineering':'Computer Science',\n",
    "    'master econometrics and operations research':'Econometrics and Operations Research',\n",
    "    'master human movement science':'Human Movement Sciences',\n",
    "    'masters computer science(big data engineering)':'Computer Science',\n",
    "    'mathematics':'Mathematics',\n",
    "    'mathematics exchange':'Exchange student',\n",
    "    'mpa':'Management, Policy-Analysis & Entrepreneurship in Health and Life Sciences',\n",
    "    'ms':np.NaN,\n",
    "    'msc ai and msc cls':'Artificial Intelligence',\n",
    "    'msc artificial intelligence':'Artificial Intelligence',\n",
    "    'msc bioinformatics':'Bioinformatics & Systems Biology',\n",
    "    'msc bioinformatics and systems biology':'Bioinformatics & Systems Biology',\n",
    "    'msc computational science':'Computational Science',\n",
    "    'msc computational science (joint degree)':'Computational Science',\n",
    "    'msc computer science':'Computer Science',\n",
    "    'msc econometrics':'Econometrics',\n",
    "    'msc. bioinformatics and systems biology':'Bioinformatics & Systems Biology',\n",
    "    'or':'Econometrics and Operations Research',\n",
    "    'phd':'PhD',\n",
    "    'phd student':'PhD',\n",
    "    'phd student at fgb':'PhD',\n",
    "    'physics':'Physics',\n",
    "    'qrm':'Quantitative Risk Management',\n",
    "    'quantitative risk management':'Quantitative Risk Management',\n",
    "    'system biology and bioinformatics':'Bioinformatics & Systems Biology'\n",
    "    })\n",
    "\n",
    "#show number of non-NaN values and number of NaN values after cleaning\n",
    "print(df.columns[1])\n",
    "print(\"#values: %d\"%(clean[df.columns[1]].count()))\n",
    "print(\"#NaN: %d\"%(clean[df.columns[1]].isnull().sum()))\n",
    "\n",
    "#show programmes sorted on occurance\n",
    "Counter(clean[df.columns[1]].dropna()).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace labelled data by integers for later classification algorithms\n",
    "clean[df.columns[2]] = df[df.columns[2]].replace({\n",
    "    'yes':1,\n",
    "    'no':0,\n",
    "    'unknown':np.NaN\n",
    "})\n",
    "\n",
    "#print number of non-NaN values and number of NaN values after cleaning\n",
    "print(df.columns[2])\n",
    "print(\"#values: %d\"%(clean[df.columns[2]].count()))\n",
    "print(\"#unknown: %d\"%(clean[df.columns[2]].isnull().sum()))\n",
    "\n",
    "#show answer sorted on occurance\n",
    "Counter(clean[df.columns[2]].dropna()).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace labelled data by integers for later classification algorithms\n",
    "clean[df.columns[3]] = df[df.columns[3]].replace({\n",
    "    '1':1,\n",
    "    '0':0,\n",
    "    'unknown':np.NaN\n",
    "})\n",
    "\n",
    "#print number of non-NaN values and number of NaN values after cleaning\n",
    "print(df.columns[3])\n",
    "print(\"#values: %d\"%(clean[df.columns[3]].count()))\n",
    "print(\"#unknown: %d\"%(clean[df.columns[3]].isnull().sum()))\n",
    "\n",
    "#show answer sorted on occurance\n",
    "Counter(clean[df.columns[3]].dropna()).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace labelled data by integers for later classification algorithms\n",
    "clean[df.columns[4]] = df[df.columns[4]].replace({\n",
    "    'mu':1,\n",
    "    'sigma':0,\n",
    "    'unknown':np.NaN\n",
    "})\n",
    "\n",
    "#print number of non-NaN values and number of NaN values after cleaning\n",
    "print(df.columns[4])\n",
    "print(\"#values: %d\"%(clean[df.columns[4]].count()))\n",
    "print(\"#unknown: %d\"%(clean[df.columns[4]].isnull().sum()))\n",
    "\n",
    "#show answer sorted on occurance\n",
    "Counter(clean[df.columns[4]].dropna()).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace labelled data by integers for later classification algorithms\n",
    "clean[df.columns[5]] = df[df.columns[5]].replace({\n",
    "    'ja':1,\n",
    "    'nee':0,\n",
    "    'unknown':np.NaN\n",
    "})\n",
    "\n",
    "#print number of non-NaN values and number of NaN values after cleaning\n",
    "print(df.columns[5])\n",
    "print(\"#values: %d\"%(clean[df.columns[5]].count()))\n",
    "print(\"#unknown: %d\"%(clean[df.columns[5]].isnull().sum()))\n",
    "\n",
    "#show answer sorted on occurance\n",
    "Counter(clean[df.columns[5]].dropna()).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace labelled data by integers for later classification algorithms\n",
    "clean[df.columns[6]] = df[df.columns[6]].replace({\n",
    "    'unknown':np.NaN,\n",
    "    'male':1,\n",
    "    'female':0\n",
    "})\n",
    "\n",
    "#print number of non-NaN values and number of NaN values after cleaning\n",
    "print(df.columns[6])\n",
    "print(\"#values: %d\"%(clean[df.columns[6]].count()))\n",
    "print(\"#unknown: %d\"%(clean[df.columns[6]].isnull().sum()))\n",
    "\n",
    "#show answer sorted on occurance\n",
    "Counter(clean[df.columns[6]].dropna()).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace labelled data by integers for later classification algorithms\n",
    "clean[df.columns[7]] = df[df.columns[7]].replace({\n",
    "    'unknown':np.NaN\n",
    "})\n",
    "\n",
    "#show answer sorted on occurance\n",
    "print(df.columns[7])\n",
    "Counter(clean[df.columns[7]].dropna()).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def clean_date(data):\n",
    "    \"\"\"\n",
    "    Cleans date in dataset.\n",
    "    \n",
    "    Assumes date in format DD-MM, MM-DD, DD-MM-YYYY, MM-DD-YYYY, YYYY-MM-DD or MMM DDth\n",
    "    Converts any other format to NaN.\n",
    "    \n",
    "    Note: may misclassify a MM-DD date as DD-MM when both DD and MM are less than 12    \n",
    "    \"\"\"\n",
    "    \n",
    "    #dict to replace month names to month number\n",
    "    monthtoyear = {'january':1,'february':2,'march':3,'april':4,'may':5,\\\n",
    "                   'june':6,'july':7,'august':8,'september':9,'october':10,\\\n",
    "                   'november':11,'december':12,'aug':8,'februari':2}\n",
    "    #replace DD/MM, DD.MM or DDth MMM notations\n",
    "    data = data.str.replace(\"/\",\"-\")\n",
    "    data = data.str.replace(\".\",\"-\")\n",
    "    data = data.str.replace(\" \",\"-\")\n",
    "    data = data.str.replace(\"th\",\"\")\n",
    "    \n",
    "    #init empty list to store cleaned dates\n",
    "    cleaned = []\n",
    "    \n",
    "    for i in data.values:\n",
    "        #try to split string into DD, MM, YYYY\n",
    "        try:\n",
    "            date = i.split(\"-\")\n",
    "        except:\n",
    "            date = np.NaN\n",
    "        \n",
    "        #convert first element, which may be DD, MMM or YYYY\n",
    "        #e.g. 28-10-1994 or March 24th etc.\n",
    "        try:\n",
    "            first = int(date[0])\n",
    "        except:\n",
    "            try:\n",
    "                first = monthtoyear[date[0].lower()]\n",
    "            except:\n",
    "                first = np.NaN\n",
    "        \n",
    "        #same for second element\n",
    "        try:\n",
    "            second = int(date[1])\n",
    "        except:\n",
    "            try:\n",
    "                second = monthtoyear[date[1].lower()]\n",
    "            except:\n",
    "                second = np.NaN\n",
    "        \n",
    "        #third is always YYYY or DD, never MMM\n",
    "        try:\n",
    "            third = int(date[2])\n",
    "        except:\n",
    "            third = np.NaN\n",
    "        \n",
    "        if second > 12 and first <= 12:\n",
    "            #assume MM-DD\n",
    "            cleaned.append(\"%02d-%02d\"%(second,first))\n",
    "        elif first <= 31 and second <= 12:\n",
    "            #assume DD-MM\n",
    "            cleaned.append(\"%02d-%02d\"%(first,second))\n",
    "        elif first > 1900 and second <= 12 and third <= 31:\n",
    "            #assume YYYY-MM-DD\n",
    "            cleaned.append(\"%02d-%02d\"%(third,second))\n",
    "        elif first > 1900 and second <= 31 and third <= 12:\n",
    "            #assume YYYY-DD-MM\n",
    "            cleaned.append(\"%02d-%02d\"%(second,third))\n",
    "        else:\n",
    "            #cannot get data, return NaN\n",
    "            cleaned.append(np.NaN)\n",
    "            \n",
    "    cleaned = pd.Series(cleaned)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "#clean dates and print number of (non)-NaN values\n",
    "print(df.columns[8])\n",
    "clean[df.columns[8]] = clean_date(df[df.columns[8]])    \n",
    "print(\"#values: %d\"%(clean[df.columns[8]].count()))\n",
    "print(\"#NaN: %d\"%(clean[df.columns[8]].isnull().sum()))\n",
    "\n",
    "#create figure\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "#drop NaN, split on '-' and obtain months only\n",
    "data = np.array(clean[df.columns[8]].dropna().str.split('-').values.tolist())[:,1]\n",
    "\n",
    "#plot birth month\n",
    "ax.hist(data.astype(int),np.arange(0.5,13.5,1),ec='black')\n",
    "ax.set_title(\"Distribution of birth month\")\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "\n",
    "#save histogram\n",
    "fig.savefig(\"dist_month.pdf\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean number of neighbours answers\n",
    "data = df[df.columns[9]]\n",
    "data = data.apply(pd.to_numeric,errors='coerce') #non-numeric -> NaN\n",
    "data = data.mask(data.lt(0) | data.gt(8))        #cannot have more than 8 neighbours!\n",
    "data = data.round()                              #can only have integer neighbours (error due to NaNs)\n",
    "clean[df.columns[9]] = data\n",
    "\n",
    "#print number of (non)-NaN values\n",
    "print(df.columns[9])\n",
    "print(\"#values: %d\"%(clean[df.columns[9]].count()))\n",
    "print(\"#NaN: %d\"%(clean[df.columns[9]].isnull().sum()))\n",
    "\n",
    "#plot histogram of distribution\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.hist(data.dropna().values, np.arange(-0.5,9.5,1), ec='black');\n",
    "ax.set_title(\"Distribution of number of neighbours\")\n",
    "ax.set_xlabel(\"Neighbours\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "\n",
    "#print number of occurance\n",
    "Counter(clean[df.columns[9]].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean column\n",
    "clean[df.columns[10]] = df[df.columns[10]].replace({\n",
    "    'yes':1,\n",
    "    'no':0,\n",
    "    'unknown':np.NaN\n",
    "})\n",
    "\n",
    "#print number of (non)-NaN values\n",
    "print(df.columns[10])\n",
    "print(\"#values: %d\"%(clean[df.columns[10]].count()))\n",
    "print(\"#NaN: %d\"%(clean[df.columns[10]].isnull().sum()))\n",
    "\n",
    "#print number of occurances\n",
    "Counter(clean[df.columns[10]].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#clean column\n",
    "data = df[df.columns[11]]\n",
    "data = data.apply(pd.to_numeric,errors='coerce') #convert values to numerical\n",
    "data = data.mask(data.lt(0) | data.gt(100))      #can only be between 0 and 100\n",
    "data = data.round(2)                             #can only get money rounded to two decimals\n",
    "clean[df.columns[11]] = data\n",
    "\n",
    "#print number of (non)-NaN values\n",
    "print(df.columns[11])\n",
    "print(\"#values: %d\"%(clean[df.columns[11]].count()))\n",
    "print(\"#NaN: %d\"%(clean[df.columns[11]].isnull().sum()))\n",
    "\n",
    "#plot distribution between £0,00 and £100,-\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "ax.hist(data.dropna().values,np.arange(-2.5,107.5,5), ec='black')\n",
    "ax.set_title(\"Distribution of money gained\")\n",
    "\n",
    "#plot distribution between £0,00 and £5,-\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "ax.hist(data.dropna().values, np.arange(-0.125,5.375,0.25), ec='black')\n",
    "ax.set_title(\"Distribution of money gained\")\n",
    "\n",
    "#show occurances\n",
    "Counter(data.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Used methods from:\n",
    "#https://stackoverflow.com/a/34844867 convert whole column to numeric\n",
    "#https://stackoverflow.com/a/41618665 mask data to NaN if outside some range\n",
    "\n",
    "#clean column\n",
    "data = df[df.columns[12]]\n",
    "data = data.str.replace('ACHT','8')\n",
    "data = data.apply(pd.to_numeric,errors='coerce') #non-numeric -> NaN\n",
    "data = data.mask(data.lt(0) | data.gt(10))       #can only be 0 - 10\n",
    "cleaned = []\n",
    "for i in data.values:\n",
    "    if np.isnan(i):\n",
    "        cleaned.append(np.NaN)\n",
    "    else:\n",
    "        cleaned.append(i)\n",
    "data = pd.Series(cleaned)\n",
    "clean[df.columns[12]] = data\n",
    "\n",
    "#print number of (non)-NaN values\n",
    "print(df.columns[12])\n",
    "print(\"#values: %d\"%(clean[df.columns[12]].count()))\n",
    "print(\"#NaN: %d\"%(clean[df.columns[12]].isnull().sum()))\n",
    "\n",
    "#plot distribution and save to file\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.hist(data.dropna().values, np.arange(-0.5,11.5,1), ec='black');\n",
    "ax.set_title(\"Random number distribution\")\n",
    "ax.set_xlabel(\"Value\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "fig.savefig(\"dist_random.pdf\",bbox_inches='tight')\n",
    "\n",
    "#print occurance\n",
    "Counter(data.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Used a method from:\n",
    "#https://stackoverflow.com/a/15321222 decoding unicode in string\n",
    "\n",
    "data = df[df.columns[13]]\n",
    "data = data.str.decode('unicode_escape').str.encode('ascii','ignore').str.decode('ascii')\n",
    "data = data.str.lower()\n",
    "data = data.str.replace(' ','')\n",
    "data = data.str.replace('a.m.','')\n",
    "data = data.str.replace('am','')\n",
    "data = data.str.replace('pm','')\n",
    "data = data.str.replace('.',':')\n",
    "data = data.str.replace('300','3:00')\n",
    "data = data.str.replace('2330','23:30')\n",
    "data = data.str.replace('2359','23:59')\n",
    "cleaned = []\n",
    "for i in data.values:\n",
    "    \n",
    "    #split hour:minute\n",
    "    time = i.split(':')\n",
    "    \n",
    "    #answered in hours or without :\n",
    "    if len(time) == 1:\n",
    "        try:\n",
    "            hour = int(time[0])\n",
    "            minute = 0\n",
    "        except:\n",
    "            hour = np.NaN\n",
    "            minute = np.NaN\n",
    "    \n",
    "    #answered in hour:minute\n",
    "    else:\n",
    "        hour = int(time[0])\n",
    "        minute = int(time[1])\n",
    "    \n",
    "    #assume 12-hour notation if time to bed between 8-12\n",
    "    if hour >= 8 and hour <= 12:\n",
    "        hour += 12\n",
    "    if hour == 24:\n",
    "        hour -= 24\n",
    "    \n",
    "    #sanity check\n",
    "    if hour > 24 or minute > 59:\n",
    "        hour = np.NaN\n",
    "        minute = np.NaN\n",
    "    \n",
    "    if np.isnan(hour):\n",
    "        cleaned.append(np.NaN)\n",
    "    else:\n",
    "        #store offset from 00:00 in hours\n",
    "        time = np.round(hour+minute/60)%24\n",
    "        if time > 6:\n",
    "            time -= 24\n",
    "        cleaned.append(time)\n",
    "clean[df.columns[13]] = pd.Series(cleaned)\n",
    "\n",
    "print(df.columns[13])\n",
    "print(\"#values: %d\"%(clean[df.columns[13]].count()))\n",
    "print(\"#NaN: %d\"%(clean[df.columns[13]].isnull().sum()))\n",
    "\n",
    "#plot distribution and save to file\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "ax.hist(clean[df.columns[13]].dropna(),np.arange(-4.5,6.5,1),ec='black')\n",
    "ax.set_title(\"Time to bed yesterday (hour)\")\n",
    "ax.set_xlabel(\"Nearest hour relative to 00:00\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "fig.savefig(\"dist_timetobed.pdf\",bbox_inches='tight')\n",
    "\n",
    "x = Counter(pd.Series(cleaned).dropna())\n",
    "sorted(x.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clean.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Basic regression/classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used methods from\n",
    "\n",
    "https://towardsdatascience.com/solving-a-simple-classification-problem-with-python-fruits-lovers-edition-d20ab6b071d2\n",
    "\n",
    "https://stackoverflow.com/a/39169661"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting programme from having taken other courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split,cross_val_predict\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = ['Have you taken a course on machine learning?',\\\n",
    "                 'Have you taken a course on information retrieval?',\\\n",
    "                 'Have you taken a course on statistics?',\\\n",
    "                 'Have you taken a course on databases?']\n",
    "classification_name = 'What programme are you in?'\n",
    "\n",
    "#only take 5 most occuring programmes as we don't have enough occurances for other\n",
    "#programmes\n",
    "subset = clean.loc[clean['What programme are you in?'].isin(['Artificial Intelligence',\\\n",
    "                                                    'Business Analytics',\\\n",
    "                                                    'Bioinformatics & Systems Biology',\n",
    "                                                    'Computational Science',\n",
    "                                                    'Computer Science'])]\n",
    "#drop NaN\n",
    "subset = subset[feature_names+[classification_name]].dropna()\n",
    "\n",
    "#split subset into features and target\n",
    "X = subset[feature_names]\n",
    "y = subset[classification_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use LogisticRegression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg=LogisticRegression()\n",
    "predicted = cross_val_predict(logreg, X, y, cv=4)\n",
    "print(metrics.classification_report(y,predicted))\n",
    "\n",
    "print(\"Accuracy score: %.2f\"%metrics.accuracy_score(y,predicted))\n",
    "print(\"Cohen Kappa score: %.2f\"%metrics.cohen_kappa_score(y,predicted))\n",
    "print(\"Confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use DecisionTree model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "predicted = cross_val_predict(clf,X,y,cv=10)\n",
    "print(metrics.classification_report(y,predicted))\n",
    "\n",
    "print(\"Accuracy score: %.2f\"%metrics.accuracy_score(y,predicted))\n",
    "print(\"Cohen Kappa score: %.2f\"%metrics.cohen_kappa_score(y,predicted))\n",
    "print(\"Confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use K-Neighbours\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "predicted = cross_val_predict(knn,X,y,cv=10)\n",
    "print(metrics.classification_report(y,predicted))\n",
    "\n",
    "print(\"Accuracy score: %.2f\"%metrics.accuracy_score(y,predicted))\n",
    "print(\"Cohen Kappa score: %.2f\"%metrics.cohen_kappa_score(y,predicted))\n",
    "print(\"Confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use Linear Discriminant Analysis\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "predicted = cross_val_predict(lda,X,y,cv=10)\n",
    "print(metrics.classification_report(y,predicted))\n",
    "\n",
    "print(\"Accuracy score: %.2f\"%metrics.accuracy_score(y,predicted))\n",
    "print(\"Cohen Kappa score: %.2f\"%metrics.cohen_kappa_score(y,predicted))\n",
    "print(\"Confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use Gaussian Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "predicted = cross_val_predict(gnb,X,y,cv=10)\n",
    "print(metrics.classification_report(y,predicted))\n",
    "\n",
    "print(\"Accuracy score: %.2f\"%metrics.accuracy_score(y,predicted))\n",
    "print(\"Cohen Kappa score: %.2f\"%metrics.cohen_kappa_score(y,predicted))\n",
    "print(\"Confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use Support Vector Classifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "predicted = cross_val_predict(svm,X,y,cv=4)\n",
    "\n",
    "print(metrics.classification_report(y,predicted))\n",
    "\n",
    "print(\"Accuracy score: %.2f\"%metrics.accuracy_score(y,predicted))\n",
    "print(\"Cohen Kappa score: %.2f\"%metrics.cohen_kappa_score(y,predicted))\n",
    "print(\"Confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict chocolate answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = ['What programme are you in?',\\\n",
    "                 'Time you went to be Yesterday',\\\n",
    "                 'What is your gender?']\n",
    "classification_name = 'Chocolate makes you.....'\n",
    "\n",
    "subset = clean[feature_names+[classification_name]].dropna()\n",
    "\n",
    "#again use five most common programmes\n",
    "subset = subset.loc[subset['What programme are you in?'].isin(['Artificial Intelligence',\\\n",
    "                                                    'Business Analytics',\\\n",
    "                                                    'Bioinformatics & Systems Biology',\n",
    "                                                    'Computational Science',\n",
    "                                                    'Computer Science'])]\n",
    "\n",
    "#remove slim answer, as occurance of this answer too small for algorithms\n",
    "subset = subset.loc[~subset['Chocolate makes you.....'].isin(['slim'])]\n",
    "\n",
    "prog_enc = {\n",
    "                         'Artificial Intelligence':0,\\\n",
    "                         'Business Analytics':1,\\\n",
    "                         'Bioinformatics & Systems Biology':2,\\\n",
    "                         'Computational Science':3,\\\n",
    "                         'Computer Science':4}\n",
    "\n",
    "subset[feature_names[0]] = subset[feature_names[0]].replace(prog_enc)\n",
    "\n",
    "\n",
    "X = subset[feature_names]\n",
    "y = subset[classification_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#https://stackoverflow.com/a/39169661\n",
    "logreg=LogisticRegression()\n",
    "predicted = cross_val_predict(logreg, X, y, cv=10)\n",
    "print(metrics.classification_report(y, predicted))\n",
    "\n",
    "print(\"Accuracy score: %.2f\"%metrics.accuracy_score(y,predicted))\n",
    "print(\"Cohen Kappa score: %.2f\"%metrics.cohen_kappa_score(y,predicted))\n",
    "print(\"Confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "predicted = cross_val_predict(clf,X,y,cv=10)\n",
    "print(metrics.classification_report(y,predicted))\n",
    "\n",
    "print(\"Accuracy score: %.2f\"%metrics.accuracy_score(y,predicted))\n",
    "print(\"Cohen Kappa score: %.2f\"%metrics.cohen_kappa_score(y,predicted))\n",
    "print(\"Confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "predicted = cross_val_predict(knn,X,y,cv=10)\n",
    "print(metrics.classification_report(y,predicted))\n",
    "\n",
    "print(\"Accuracy score: %.2f\"%metrics.accuracy_score(y,predicted))\n",
    "print(\"Cohen Kappa score: %.2f\"%metrics.cohen_kappa_score(y,predicted))\n",
    "print(\"Confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "predicted = cross_val_predict(lda,X,y,cv=10)\n",
    "print(metrics.classification_report(y,predicted))\n",
    "\n",
    "print(\"Accuracy score: %.2f\"%metrics.accuracy_score(y,predicted))\n",
    "print(\"Cohen Kappa score: %.2f\"%metrics.cohen_kappa_score(y,predicted))\n",
    "print(\"Confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "predicted = cross_val_predict(gnb,X,y,cv=10)\n",
    "print(metrics.classification_report(y,predicted))\n",
    "\n",
    "print(\"Accuracy score: %.2f\"%metrics.accuracy_score(y,predicted))\n",
    "print(\"Cohen Kappa score: %.2f\"%metrics.cohen_kappa_score(y,predicted))\n",
    "print(\"Confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "predicted = cross_val_predict(svm,X,y,cv=10)\n",
    "print(metrics.classification_report(y,predicted))\n",
    "\n",
    "print(\"Accuracy score: %.2f\"%metrics.accuracy_score(y,predicted))\n",
    "print(\"Cohen Kappa score: %.2f\"%metrics.cohen_kappa_score(y,predicted))\n",
    "print(\"Confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Predict greediness based on programme and gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_names = ['What programme are you in?',\n",
    "                 'What is your gender?']\n",
    "classification_name = clean.columns[11]\n",
    "\n",
    "subset = clean[feature_names+[classification_name]].dropna()\n",
    "\n",
    "subset.loc[subset[classification_name]<=1,classification_name] = 0 #not greedy\n",
    "subset.loc[subset[classification_name]>1,classification_name] = 1  #greedy\n",
    "\n",
    "subset = subset.loc[subset['What programme are you in?'].isin(['Artificial Intelligence',\\\n",
    "                                                    'Business Analytics',\\\n",
    "                                                    'Bioinformatics & Systems Biology',\n",
    "                                                    'Computational Science',\n",
    "                                                    'Computer Science'])]\n",
    "prog_enc = {\n",
    "                         'Artificial Intelligence':0,\\\n",
    "                         'Business Analytics':1,\\\n",
    "                         'Bioinformatics & Systems Biology':2,\\\n",
    "                         'Computational Science':3,\\\n",
    "                         'Computer Science':4}\n",
    "\n",
    "subset[feature_names[0]] = subset[feature_names[0]].replace(prog_enc)\n",
    "\n",
    "X = subset[feature_names]\n",
    "y = subset[classification_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg=LogisticRegression()\n",
    "predicted = cross_val_predict(logreg, X, y, cv=10)\n",
    "print(metrics.classification_report(y, predicted))\n",
    "\n",
    "print(\"Accuracy score: %.2f\"%metrics.accuracy_score(y,predicted))\n",
    "print(\"Cohen Kappa score: %.2f\"%metrics.cohen_kappa_score(y,predicted))\n",
    "print(\"Confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "predicted = cross_val_predict(clf,X,y,cv=10)\n",
    "print(metrics.classification_report(y,predicted))\n",
    "\n",
    "print(\"Accuracy score: %.2f\"%metrics.accuracy_score(y,predicted))\n",
    "print(\"Cohen Kappa score: %.2f\"%metrics.cohen_kappa_score(y,predicted))\n",
    "print(\"Confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "predicted = cross_val_predict(knn,X,y,cv=10)\n",
    "print(metrics.classification_report(y,predicted))\n",
    "\n",
    "print(\"Accuracy score: %.2f\"%metrics.accuracy_score(y,predicted))\n",
    "print(\"Cohen Kappa score: %.2f\"%metrics.cohen_kappa_score(y,predicted))\n",
    "print(\"Confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "predicted = cross_val_predict(lda,X,y,cv=10)\n",
    "print(metrics.classification_report(y,predicted))\n",
    "\n",
    "print(\"Accuracy score: %.2f\"%metrics.accuracy_score(y,predicted))\n",
    "print(\"Cohen Kappa score: %.2f\"%metrics.cohen_kappa_score(y,predicted))\n",
    "print(\"Confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "predicted = cross_val_predict(gnb,X,y,cv=10)\n",
    "print(metrics.classification_report(y,predicted))\n",
    "\n",
    "print(\"Accuracy score: %.2f\"%metrics.accuracy_score(y,predicted))\n",
    "print(\"Cohen Kappa score: %.2f\"%metrics.cohen_kappa_score(y,predicted))\n",
    "print(\"Confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "predicted = cross_val_predict(svm,X,y,cv=10)\n",
    "print(metrics.classification_report(y,predicted))\n",
    "\n",
    "print(\"Accuracy score: %.2f\"%metrics.accuracy_score(y,predicted))\n",
    "print(\"Cohen Kappa score: %.2f\"%metrics.cohen_kappa_score(y,predicted))\n",
    "print(\"Confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y,predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
